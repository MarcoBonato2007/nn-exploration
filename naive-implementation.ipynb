{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was my first implementation of a neural network. Some important things to note:\n",
    "    # HORRIBLY INEFFICIENT. I was in my mid-early years of high school making this, and \n",
    "    # i didn't know about matrices, and i actually used forward and not backward propagation.\n",
    "\n",
    "    # By forward propagation, i mean that, to perform gradient descent, I kept track of each \n",
    "    # partial derivative separately and went forward through the network, summing \n",
    "    # the partial derivatives at the end.\n",
    "\n",
    "    # I did this because I didn't know that you were meant to use backward propagation.\n",
    "    # I only knew basic theory about derivatives, and tried to figure the rest out myself.\n",
    "\n",
    "    # That led to this monstrosity. \n",
    "    # Check the other file for a 10000x better numpy implementation.\n",
    "\n",
    "    # As a note, I'm actually glad I did my neural network this way at first.\n",
    "    # It taught me a lot about neural networks and introduced me to matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbf601",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T17:30:45.079642Z",
     "iopub.status.busy": "2023-10-17T17:30:45.079143Z",
     "iopub.status.idle": "2023-10-17T17:30:45.085994Z",
     "shell.execute_reply": "2023-10-17T17:30:45.084720Z"
    },
    "papermill": {
     "duration": 0.01283,
     "end_time": "2023-10-17T17:30:45.088059",
     "exception": false,
     "start_time": "2023-10-17T17:30:45.075229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30b951d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-17T17:30:45.094284Z",
     "iopub.status.busy": "2023-10-17T17:30:45.094016Z",
     "iopub.status.idle": "2023-10-17T17:30:45.116156Z",
     "shell.execute_reply": "2023-10-17T17:30:45.114774Z"
    },
    "papermill": {
     "duration": 0.027664,
     "end_time": "2023-10-17T17:30:45.118243",
     "exception": false,
     "start_time": "2023-10-17T17:30:45.090579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, sizes, activation_functions, leak=0.1):\n",
    "        \n",
    "        # Initialize weights randomly, initialize biases as 0\n",
    "        self.weights = [[[random.random() for i in range(sizes[i])] for j in range(v)] for i,v in enumerate(sizes[1:])]\n",
    "        self.biases = [[1]*v for v in sizes[1:]]\n",
    "        self.activation_functions = activation_functions   \n",
    "        self.leak = leak\n",
    "        \n",
    "        self.func_dict = {\n",
    "            \"sigmoid\": lambda x: 1/(1+math.e**(-x)),\n",
    "            \"relu\": lambda x: max(0, x),\n",
    "            \"leaky_relu\": lambda x: max(self.leak*x, x),\n",
    "            \"tanh\": lambda x: 2/(1+math.e**(-2*x))-1,\n",
    "            \"mean_squared_error\": lambda a, y: (a-y)**2\n",
    "        }\n",
    "        \n",
    "        self.deriv_dict = {\n",
    "            \"sigmoid\": lambda x: 1/(1+math.e**(-x)) * (1 - 1/(1+math.e**(-x))),\n",
    "            \"relu\": lambda x: 1*(x>0),\n",
    "            \"leaky_relu\": lambda x: 1*(x>0)+self.leak*(x<0),\n",
    "            \"tanh\": lambda x: 1- (2/(1+math.e**(-2*(2/(1+math.e**(-2*x))-1)))-1),\n",
    "            \"mean_squared_error\": lambda a, y: 2*(a-y)\n",
    "        }\n",
    "\n",
    "        \n",
    "    def forward_pass(self, input_activations):\n",
    "        \n",
    "        output = [[[None, i] for i in input_activations]]\n",
    "        \n",
    "        for li, l in enumerate(self.weights):\n",
    "            activation_function = self.func_dict[self.activation_functions[li]]\n",
    "            output.append([])\n",
    "            for ni, n in enumerate(l):\n",
    "                z = sum([w*input_activations[wi] for wi, w in enumerate(n)]) + self.biases[li][ni]\n",
    "                a = activation_function(z)\n",
    "                output[-1].append([z, a])\n",
    "            input_activations = [i[1] for i in output[-1]]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def output(self, input_activations):\n",
    "        return [i[1] for i in self.forward_pass(input_activations)[-1]]\n",
    "    \n",
    "    def backprop(self, x, y, loss=\"mean_squared_error\", learning_rate=1):\n",
    "        output = self.forward_pass(x)\n",
    "        weight_calculations = copy.deepcopy(self.weights)\n",
    "        bias_calculations = copy.deepcopy(self.biases)\n",
    "        \n",
    "        # li means layer index, l means layer\n",
    "        # ni means neuron index, n means neuron\n",
    "        # This usually refers to the weights connected to a neuron from the previous layer\n",
    "        # wi means weight index, w means weight\n",
    "        # bi means bias index, b means bias\n",
    "        # ci means calculation index, c means calculation\n",
    "        # This means the different possible derivatives for a weight or bias\n",
    "        # The letter p in front of these prefices means \"previous\"\n",
    "        \n",
    "        # For each layer, \n",
    "        for li, l in enumerate(weight_calculations):\n",
    "            \n",
    "            # Update weight calculations in previous layers\n",
    "            for pli, pl in enumerate(weight_calculations[:li]):\n",
    "                for pni, pn in enumerate(pl):\n",
    "                    for pwi, pw in enumerate(pn):\n",
    "                        # Checks if it is a single calculation or multiple.\n",
    "                        if type(pw) == float or type(pw) == int or type(pw) == np.float64:\n",
    "                            weight_calculations[pli][pni][pwi] = [pw]\n",
    "                            for ni, n in enumerate(l):\n",
    "                                w = weight_calculations[li][ni][pni]\n",
    "                                activation_function = self.activation_functions[li]\n",
    "                                activation_deriv = self.deriv_dict[activation_function]\n",
    "                                z = output[li+1][ni][0]\n",
    "                                weight_calculations[pli][pni][pwi].append(pw*w*activation_deriv(z))\n",
    "                            del weight_calculations[pli][pni][pwi][0]\n",
    "                        else:\n",
    "                            pw = pw.copy()\n",
    "                            for ci, c in enumerate(pw):\n",
    "                                origin = int(ci//(len(pw)/len(weight_calculations[li-1])))\n",
    "                                for ni, n in enumerate(l):\n",
    "                                    w = weight_calculations[li][ni][origin]\n",
    "                                    activation_function = self.activation_functions[li]\n",
    "                                    activation_deriv = self.deriv_dict[activation_function]\n",
    "                                    z = output[li+1][ni][0]\n",
    "                                    weight_calculations[pli][pni][pwi].append(c*w*activation_deriv(z))\n",
    "                            del weight_calculations[pli][pni][pwi][0:len(pw)]\n",
    "\n",
    "                            \n",
    "            # Initialize calculation for weights in the current layer\n",
    "            for ni, n in enumerate(l):\n",
    "                for wi, w in enumerate(n):\n",
    "                    activation_function = self.activation_functions[li]\n",
    "                    activation_deriv = self.deriv_dict[activation_function]\n",
    "                    z = output[li+1][ni][0]\n",
    "                    a = output[li][wi][1]\n",
    "                    weight_calculations[li][ni][wi] = a*activation_deriv(z)\n",
    "        \n",
    "    \n",
    "            # Update bias calculations in previous layers\n",
    "            for pli, pl in enumerate(bias_calculations[:li]):\n",
    "                for pbi, pb in enumerate(pl):\n",
    "                    if type(pb) == float or type(pb) == int or type(pb) == np.float64:\n",
    "                        bias_calculations[pli][pbi] = [pb]\n",
    "                        for ni, n in enumerate(l):\n",
    "                            w = self.weights[li][ni][pbi]\n",
    "                            activation_function = self.activation_functions[li]\n",
    "                            activation_deriv = self.deriv_dict[activation_function]\n",
    "                            z = output[li+1][ni][0]\n",
    "                            bias_calculations[pli][pbi].append(pb*w*activation_deriv(z))\n",
    "                        del bias_calculations[pli][pbi][0]\n",
    "                    else:\n",
    "                        pb = pb.copy()\n",
    "                        for ci, c in enumerate(pb):\n",
    "                            origin = int(ci//(len(pb)/len(self.weights[li-1])))\n",
    "                            for ni, n in enumerate(l):\n",
    "                                w = self.weights[li][ni][origin]\n",
    "                                activation_function = self.activation_functions[li]\n",
    "                                activation_deriv = self.deriv_dict[activation_function]\n",
    "                                z = output[li+1][ni][0]\n",
    "                                bias_calculations[pli][pbi].append(c*w*activation_deriv(z))\n",
    "                        del bias_calculations[pli][pbi][0:len(pb)]\n",
    "\n",
    "                        \n",
    "            # Initialize calculation for biases in the current layer\n",
    "            for bi, b in enumerate(bias_calculations[li]):\n",
    "                activation_function = self.activation_functions[li]\n",
    "                activation_deriv = self.deriv_dict[activation_function]\n",
    "                z = output[li+1][bi][0]\n",
    "                bias_calculations[li][bi] = activation_deriv(z)\n",
    "                                        \n",
    "        \n",
    "    #         Take sum of paths for weights\n",
    "#             Multiply by deriv of loss\n",
    "        loss_deriv = self.deriv_dict[loss]\n",
    "        for li, l in enumerate(weight_calculations):\n",
    "            for ni, n in enumerate(l):\n",
    "                for wi, w in enumerate(n):\n",
    "                    if type(w) == float or type(w) == int or type(w) == np.float64:\n",
    "                        y_pred = output[li+1][ni][1]\n",
    "                        y_true = y[ni]\n",
    "                        weight_calculations[li][ni][wi] *= loss_deriv(y_pred, y_true)\n",
    "                    else:\n",
    "                        for ci, c in enumerate(w):\n",
    "                            origin = int(ci//(len(w)/len(weight_calculations[-1])))\n",
    "                            y_pred = output[-1][origin][1]\n",
    "                            y_true = y[origin]\n",
    "                            weight_calculations[li][ni][wi][ci] *= loss_deriv(y_pred, y_true)\n",
    "                        weight_calculations[li][ni][wi] = sum(weight_calculations[li][ni][wi])\n",
    "                    # Minus derivatives from the weights\n",
    "                    self.weights[li][ni][wi] -= weight_calculations[li][ni][wi]*learning_rate\n",
    "                    \n",
    "        \n",
    "#         Take sum of paths for biases\n",
    "#             Multiply by deriv of loss\n",
    "        for li, l in enumerate(bias_calculations):\n",
    "            for bi, b in enumerate(l):\n",
    "                if type(b) == float or type(b) == int or type(b) == np.float64:\n",
    "                    y_pred = output[li+1][bi][1]\n",
    "                    y_true = y[bi]\n",
    "                    bias_calculations[li][bi] *= loss_deriv(y_pred, y_true)\n",
    "                else:\n",
    "                    for ci, c in enumerate(b):\n",
    "                        origin = int(ci//(len(b)/len(weight_calculations[-1])))\n",
    "                        y_pred = output[-1][origin][1]\n",
    "                        y_true = y[origin]\n",
    "                        bias_calculations[li][bi][ci] *= loss_deriv(y_pred, y_true)\n",
    "                    bias_calculations[li][bi] = sum(weight_calculations[li][bi])\n",
    "                # Minus derivatives from the biases\n",
    "                self.biases[li][bi] -= bias_calculations[li][bi]*learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2e7f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T17:30:45.124233Z",
     "iopub.status.busy": "2023-10-17T17:30:45.123925Z",
     "iopub.status.idle": "2023-10-17T17:30:45.128225Z",
     "shell.execute_reply": "2023-10-17T17:30:45.126938Z"
    },
    "papermill": {
     "duration": 0.009304,
     "end_time": "2023-10-17T17:30:45.129910",
     "exception": false,
     "start_time": "2023-10-17T17:30:45.120606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train a tiny neural net to translate [1, 3] into [0.1, 0.5]\n",
    "model = Perceptron([2, 2, 2, 2], [\"leaky_relu\", \"leaky_relu\", \"sigmoid\"])\n",
    "for i in range(100): \n",
    "    model.backprop([1, 3], [0.1, 0.5])\n",
    "model.output([1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae6cb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T17:30:45.206213Z",
     "iopub.status.busy": "2023-10-17T17:30:45.205952Z",
     "iopub.status.idle": "2023-10-17T17:30:45.211597Z",
     "shell.execute_reply": "2023-10-17T17:30:45.210578Z"
    },
    "papermill": {
     "duration": 0.010648,
     "end_time": "2023-10-17T17:30:45.213262",
     "exception": false,
     "start_time": "2023-10-17T17:30:45.202614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here I attempted to run my neural net on the full MNIST dataset\n",
    "# Because this was so inefficient, i realized i did my implementation very inefficiently\n",
    "\n",
    "model = Perceptron([784, 16, 16, 10], [\"leaky_relu\", \"leaky_relu\", \"sigmoid\"])\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "for i,v in enumerate(list(train[\"label\"])):\n",
    "    y = [0]*(v)+[1]+[0]*(9-v)\n",
    "    x = list(train.iloc[i])[1:]\n",
    "    model.backprop(x, y)\n",
    "    break # Break statement here put so you don't waste too much time..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.07377,
   "end_time": "2023-10-17T17:30:54.165919",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-17T17:30:36.092149",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
