{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was my much newer, much improved, more mathematically sound implementation\n",
    "# of a neural network. I've used proper numpy and matrices here.\n",
    "\n",
    "# Forgive the bad naming: I was programming from a mathematical perspective\n",
    "# with single-letter variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of some of the neural network variables\n",
    "\n",
    "n = np.array([784, 16, 16, 10]) # Layer sizes, incl input layer\n",
    "L = len(n)-1 # number of layers, excluding input layer\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "X = train.iloc[:, 1:].to_numpy().T/255 # inputs\n",
    "Y = train[\"label\"].to_numpy() # expected outputs\n",
    "one_hot_Y = np.zeros((Y.size, 10))\n",
    "one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "Y = one_hot_Y.T\n",
    "\n",
    "N = X.shape[1] # Number of training examples\n",
    "\n",
    "A = [np.zeros((n[i], N)) for i in range(L+1)] # activation values at each layer (incl. input layer)\n",
    "A[0] = X # first layer activations = input values\n",
    "\n",
    "# We add some None's to be able to start indexing at 1\n",
    "Z = [None] + [np.zeros((n[i], N)) for i in range(1, L+1)] # Start z values (intermediate weighted-sum + bias) at 0\n",
    "W = [None] # Weights\n",
    "B = [None] # Biases\n",
    "for i in range(L):\n",
    "    # Initialize weights and biases between -0.5 and 0.5\n",
    "    W.append(np.random.rand(n[i+1], n[i]) - 0.5)\n",
    "    B.append(np.random.rand(n[i+1], 1) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare activation functions\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    epsilon = 10**-7\n",
    "    return np.exp(x)/(sum(np.exp(x))+epsilon)\n",
    "\n",
    "def softmax_deriv(x):\n",
    "    return softmax(x)*(1-softmax(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return x > 0\n",
    "\n",
    "class ActivationFunction:\n",
    "    def __init__(self, function, derivative):\n",
    "        self.activation = function\n",
    "        self.derivative = derivative\n",
    "\n",
    "# Activation functions at each layer\n",
    "F = [\n",
    "    None, # None so we can index starting at 1\n",
    "    ActivationFunction(relu, relu_deriv),\n",
    "    ActivationFunction(relu, relu_deriv),\n",
    "    ActivationFunction(softmax, softmax_deriv),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(Z, A, W, B, F):\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + B[i]\n",
    "        A[i] = F[i].activation(Z[i])\n",
    "    return Z, A\n",
    "\n",
    "def output(Z, A, W, B, F):\n",
    "    Z, A = forward_prop(Z, A, W, B, F)\n",
    "    return A[-1]\n",
    "\n",
    "def error(A, Y):\n",
    "    # We're using mean squared error as our error function\n",
    "    square = lambda x: x**2\n",
    "    return np.sum(square(A[-1]-Y)) # A[-1] used to get last layer\n",
    "\n",
    "def error_deriv(A, Y): # derivative of error function\n",
    "    return 2*(A[-1]-Y)\n",
    "\n",
    "def backprop(Z, A, Y, W, B, F, N, learning_rate):\n",
    "    Z, A = forward_prop(Z, A, W, B, F)\n",
    "    dEdA = error_deriv(A, Y) # dE/dA = Derivative of error with respect to last layer activations\n",
    "    for i in range(L, 0, -1): # go backwards through layer indices (backpropagation)\n",
    "        dAdZ = F[i].derivative(Z[i])\n",
    "        dEdZ = dEdA*dAdZ\n",
    "        dEdA = np.matmul(W[i].T, dEdZ)\n",
    "        B[i] -= learning_rate*1/N*np.sum(dEdZ, axis=1, keepdims=True) # dZdB = 1\n",
    "        W[i] -= learning_rate*1/N*np.dot(dEdZ, A[i-1].T)\n",
    "\n",
    "    return B, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "num_backprops = 1000\n",
    "for i in range(1000):\n",
    "    if i%100 == 0: \n",
    "        print(f\"{i}/{num_backprops} backpropagations done\")\n",
    "    B, W = backprop(Z, A, Y, W, B, F, N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 7 6 9]\n"
     ]
    }
   ],
   "source": [
    "out = np.argmax(output(Z, A, W, B, F).T, axis=1)\n",
    "print(out) # a row vector of all the MNIST digit predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9275714285714286\n"
     ]
    }
   ],
   "source": [
    "# Get the accuracy\n",
    "print(np.sum(np.argmax(output(Z, A, W, B, F).T, axis=1) == train[\"label\"])/X.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
